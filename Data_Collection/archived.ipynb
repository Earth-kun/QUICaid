{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data transformation\n",
    "import pandas as pd            \n",
    "# For statistical analysis\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "# For ASN lookup\n",
    "import pyasn\n",
    "asndb = pyasn.pyasn('ipasn_20140513.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input csv\n",
    "input_file = \"./cesnet_dataset.csv\"\n",
    "new_df = pd.read_csv(input_file, sep=\",\", low_memory=False, header= 0)\n",
    "#df = pd.read_csv(input_file, dtype={\"DURATION\": float, \"SRC_IP\": str, \"DST_IP\": str, \"SRC_PORT\": int, \"DST_PORT\": int}, sep=\",\", low_memory=False, header= 0)\n",
    "\n",
    "# delete protocol column\n",
    "# df = df.drop(columns=[\"PROTOCOL\"], axis=1)\n",
    "# df[\"BYTES\"] = pd.to_numeric(df[\"BYTES\"], errors='coerce').fillna(0)\n",
    "\n",
    "# Read the existing CSV file\n",
    "output_file = \"./benign_flow/benign.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00%\n",
      "6.96%\n",
      "13.91%\n",
      "20.87%\n",
      "27.83%\n",
      "34.79%\n",
      "41.74%\n",
      "48.70%\n",
      "55.66%\n",
      "62.62%\n",
      "69.57%\n",
      "76.53%\n",
      "83.49%\n",
      "90.45%\n",
      "97.40%\n"
     ]
    }
   ],
   "source": [
    "# set source IP, web service category, and label\n",
    "ipsrc = \"10.10.3.10\"\n",
    "label = \"0\"\n",
    "quic_ver = \"1\"\n",
    "\n",
    "# define the list for each feature\n",
    "df_dst_port = []\n",
    "df_dst_asn = []\n",
    "df_dur = []\n",
    "df_ratio = []\n",
    "df_flow_pkt = []\n",
    "df_flow_bytes = []\n",
    "df_tot_pkt = []\n",
    "df_tot_bytes = []\n",
    "df_max_bytes = []\n",
    "df_min_bytes = []\n",
    "df_ave_bytes = []\n",
    "df_std_bytes = []\n",
    "df_var_bytes = []\n",
    "df_fwd_pkt = []\n",
    "df_fwd_bytes = []\n",
    "df_max_fwd_bytes = []\n",
    "df_min_fwd_bytes = []\n",
    "df_ave_fwd_bytes = []\n",
    "df_std_fwd_bytes = []\n",
    "df_var_fwd_bytes = []\n",
    "df_rev_pkt = []\n",
    "df_rev_bytes = []\n",
    "df_max_rev_bytes = []\n",
    "df_min_rev_bytes = []\n",
    "df_ave_rev_bytes = []\n",
    "df_std_rev_bytes = []\n",
    "df_var_rev_bytes = []\n",
    "df_max_iat = []\n",
    "df_min_iat = []\n",
    "df_ave_iat = []\n",
    "df_std_iat = []\n",
    "df_var_iat = []\n",
    "df_fwd_dur = []\n",
    "df_max_fwd_iat = []\n",
    "df_min_fwd_iat = []\n",
    "df_ave_fwd_iat = []\n",
    "df_std_fwd_iat = []\n",
    "df_var_fwd_iat = []\n",
    "df_rev_dur = []\n",
    "df_max_rev_iat = []\n",
    "df_min_rev_iat = []\n",
    "df_ave_rev_iat = []\n",
    "df_std_rev_iat = []\n",
    "df_var_rev_iat = []\n",
    "df_label = []\n",
    "\n",
    "# initialize the interpacket variables\n",
    "arr_port = []\n",
    "arr_asn = []\n",
    "arr_ver = []\n",
    "def initialize_variables():\n",
    "    global arr_fwd_bytes, arr_rev_bytes, arr_fwd_iat, arr_rev_iat, arr_port, arr_asn, arr_ver\n",
    "    global fwd_pkt, fwd_bytes, rev_pkt, rev_bytes, init_dur, dur, fwd_dur, rev_dur, ctr\n",
    "    arr_fwd_bytes = []\n",
    "    arr_rev_bytes = []\n",
    "    arr_fwd_iat = []\n",
    "    arr_rev_iat = []\n",
    "    fwd_pkt = 0\n",
    "    fwd_bytes = 0.0\n",
    "    rev_pkt = 0\n",
    "    rev_bytes = 0.0\n",
    "    init_dur = 0.0\n",
    "    dur = 0.0\n",
    "    fwd_dur = 0.0\n",
    "    rev_dur = 0.0\n",
    "    ctr = 0\n",
    "\n",
    "def get_asn(ip):\n",
    "    try:\n",
    "        return asndb.lookup(ip)[0]\n",
    "    except:\n",
    "        false_ip = ip.split(\",\")[0]\n",
    "        return asndb.lookup(false_ip)[0]\n",
    "\n",
    "initialize_variables()\n",
    "for index, row in df.iterrows():\n",
    "    value = row['SRC_IP']\n",
    "    if ctr == 29 or index == len(df) - 1 or (index > 0 and abs(df.at[index, 'DURATION'] - df.at[index - 1, 'DURATION']) > 1.0):\n",
    "        if value.startswith(ipsrc):\n",
    "            fwd_dur += (row['DURATION'] - init_dur)\n",
    "            arr_fwd_iat.append(row['DURATION'] - init_dur)\n",
    "            arr_port.append(row['DST_PORT'])\n",
    "            arr_asn.append(get_asn(row['DST_IP']))\n",
    "            arr_ver.append(row['QUIC_VERSION'])\n",
    "            fwd_pkt += 1\n",
    "            fwd_bytes += row['BYTES']\n",
    "            arr_fwd_bytes.append(row['BYTES'])\n",
    "        else:\n",
    "            rev_dur += (row['DURATION'] - init_dur)\n",
    "            arr_rev_iat.append(row['DURATION'] - init_dur)\n",
    "            arr_port.append(row['SRC_PORT'])\n",
    "            arr_asn.append(get_asn(row['SRC_IP']))\n",
    "            arr_ver.append(row['QUIC_VERSION'])\n",
    "            rev_pkt += 1\n",
    "            rev_bytes += row['BYTES']\n",
    "            arr_rev_bytes.append(row['BYTES'])\n",
    "\n",
    "        dst_port = stats.mode(arr_port)\n",
    "        dst_asn = stats.mode([x for x in arr_asn if isinstance(x, int)])\n",
    "        new_quic_ver = stats.mode(arr_ver)\n",
    "\n",
    "        if isinstance(new_quic_ver, tuple):\n",
    "            quic_ver = max([int(h) for h in new_quic_ver])\n",
    "        elif isinstance(new_quic_ver, int):\n",
    "            quic_ver = [int(new_quic_ver)]\n",
    "        \n",
    "        dur += (row['DURATION'] - init_dur)\n",
    "        ratio = 1 if rev_pkt > fwd_pkt else 0\n",
    "        tot_pkt = fwd_pkt + rev_pkt\n",
    "        tot_bytes = fwd_bytes + rev_bytes\n",
    "        flow_pkt = tot_pkt / dur\n",
    "        flow_bytes = tot_bytes / dur\n",
    "        \n",
    "        if len(arr_fwd_bytes) > 0 and len(arr_rev_bytes) > 0:\n",
    "            combined_bytes = np.array(arr_fwd_bytes + arr_rev_bytes)\n",
    "            max_bytes = np.max(combined_bytes)\n",
    "            min_bytes = np.min(combined_bytes)\n",
    "            ave_bytes = np.mean(combined_bytes)\n",
    "            std_bytes = np.std(combined_bytes)\n",
    "            var_bytes = np.var(combined_bytes)\n",
    "\n",
    "        if len(arr_fwd_bytes) > 0:\n",
    "            max_fwd_bytes = np.max(arr_fwd_bytes)\n",
    "            min_fwd_bytes = np.min(arr_fwd_bytes)\n",
    "            ave_fwd_bytes = np.mean(arr_fwd_bytes)\n",
    "            std_fwd_bytes = np.std(arr_fwd_bytes)\n",
    "            var_fwd_bytes = np.var(arr_fwd_bytes)\n",
    "\n",
    "        if len(arr_rev_bytes) > 0:\n",
    "            max_rev_bytes = np.max(arr_rev_bytes)\n",
    "            min_rev_bytes = np.min(arr_rev_bytes)\n",
    "            ave_rev_bytes = np.mean(arr_rev_bytes)\n",
    "            std_rev_bytes = np.std(arr_rev_bytes)\n",
    "            var_rev_bytes = np.var(arr_rev_bytes)\n",
    "\n",
    "        if len(arr_fwd_iat) > 0 and len(arr_rev_iat) > 0:\n",
    "            combined_iat = np.array(arr_fwd_iat + arr_rev_iat)\n",
    "            max_iat = np.max(combined_iat)\n",
    "            min_iat = np.min(combined_iat)\n",
    "            ave_iat = np.mean(combined_iat)\n",
    "            std_iat = np.std(combined_iat)\n",
    "            var_iat = np.var(combined_iat)\n",
    "\n",
    "        if len(arr_fwd_iat) > 0:\n",
    "            max_fwd_iat = np.max(arr_fwd_iat)\n",
    "            min_fwd_iat = np.min(arr_fwd_iat)\n",
    "            ave_fwd_iat = np.mean(arr_fwd_iat)\n",
    "            std_fwd_iat = np.std(arr_fwd_iat)\n",
    "            var_fwd_iat = np.var(arr_fwd_iat)\n",
    "\n",
    "        if len(arr_rev_iat) > 0:\n",
    "            max_rev_iat = np.max(arr_rev_iat)\n",
    "            min_rev_iat = np.min(arr_rev_iat)\n",
    "            ave_rev_iat = np.mean(arr_rev_iat)\n",
    "            std_rev_iat = np.std(arr_rev_iat)\n",
    "            var_rev_iat = np.var(arr_rev_iat)\n",
    "\n",
    "        df_dst_port.append(dst_port)\n",
    "        df_dst_asn.append(dst_asn)\n",
    "        df_dur.append(dur)\n",
    "        df_ratio.append(ratio)\n",
    "        df_flow_pkt.append(flow_pkt)\n",
    "        df_flow_bytes.append(flow_bytes)\n",
    "        df_tot_pkt.append(tot_pkt)\n",
    "        df_tot_bytes.append(tot_bytes)\n",
    "        df_max_bytes.append(max_bytes)\n",
    "        df_min_bytes.append(min_bytes)\n",
    "        df_ave_bytes.append(ave_bytes)\n",
    "        df_std_bytes.append(std_bytes)\n",
    "        df_var_bytes.append(var_bytes)\n",
    "        df_fwd_pkt.append(fwd_pkt)\n",
    "        df_fwd_bytes.append(fwd_bytes)\n",
    "        df_max_fwd_bytes.append(max_fwd_bytes)\n",
    "        df_min_fwd_bytes.append(min_fwd_bytes)\n",
    "        df_ave_fwd_bytes.append(ave_fwd_bytes)\n",
    "        df_std_fwd_bytes.append(std_fwd_bytes)\n",
    "        df_var_fwd_bytes.append(var_fwd_bytes)\n",
    "        df_rev_pkt.append(rev_pkt)\n",
    "        df_rev_bytes.append(rev_bytes)\n",
    "        df_max_rev_bytes.append(max_rev_bytes)\n",
    "        df_min_rev_bytes.append(min_rev_bytes)\n",
    "        df_ave_rev_bytes.append(ave_rev_bytes)\n",
    "        df_std_rev_bytes.append(std_rev_bytes)\n",
    "        df_var_rev_bytes.append(var_rev_bytes)\n",
    "        df_max_iat.append(max_iat)\n",
    "        df_min_iat.append(min_iat)\n",
    "        df_ave_iat.append(ave_iat)\n",
    "        df_std_iat.append(std_iat)\n",
    "        df_var_iat.append(var_iat)\n",
    "        df_fwd_dur.append(fwd_dur)\n",
    "        df_max_fwd_iat.append(max_fwd_iat)\n",
    "        df_min_fwd_iat.append(min_fwd_iat)\n",
    "        df_ave_fwd_iat.append(ave_fwd_iat)\n",
    "        df_std_fwd_iat.append(std_fwd_iat)\n",
    "        df_var_fwd_iat.append(var_fwd_iat)\n",
    "        df_rev_dur.append(rev_dur)\n",
    "        df_max_rev_iat.append(max_rev_iat)\n",
    "        df_min_rev_iat.append(min_rev_iat)\n",
    "        df_ave_rev_iat.append(ave_rev_iat)\n",
    "        df_std_rev_iat.append(std_rev_iat)\n",
    "        df_var_rev_iat.append(var_rev_iat)\n",
    "        df_label.append(label) \n",
    "\n",
    "        initialize_variables()\n",
    "    elif value.startswith(ipsrc) and ctr < 29:\n",
    "        if ctr == 0:\n",
    "            init_dur = row['DURATION']\n",
    "        else:\n",
    "            fwd_dur += (row['DURATION'] - init_dur)\n",
    "            arr_fwd_iat.append(row['DURATION'] - init_dur)\n",
    "        ctr += 1\n",
    "        arr_port.append(row['DST_PORT'])\n",
    "        arr_asn.append(get_asn(row['DST_IP']))\n",
    "        arr_ver.append(row['QUIC_VERSION'])\n",
    "        fwd_pkt += 1\n",
    "        fwd_bytes += row['BYTES']\n",
    "        arr_fwd_bytes.append(row['BYTES'])\n",
    "    elif value.startswith(ipsrc) == False and ctr < 29:\n",
    "        if ctr == 0:\n",
    "            init_dur = row['DURATION']\n",
    "        else:\n",
    "            rev_dur += (row['DURATION'] - init_dur)\n",
    "            arr_rev_iat.append(row['DURATION'] - init_dur)\n",
    "        ctr += 1\n",
    "        arr_port.append(row['SRC_PORT'])\n",
    "        arr_asn.append(get_asn(row['SRC_IP']))\n",
    "        arr_ver.append(row['QUIC_VERSION'])\n",
    "        rev_pkt += 1\n",
    "        rev_bytes += row['BYTES']\n",
    "        arr_rev_bytes.append(row['BYTES'])\n",
    "    \n",
    "    # track progress in percent with respect to total rows\n",
    "    if index % 1000 == 0:\n",
    "        print(f\"{index / len(df) * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7228974\n"
     ]
    }
   ],
   "source": [
    "# for item in flow:\n",
    "#     print(item)\n",
    "\n",
    "print(len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all flow records\n",
    "flow_rows = []\n",
    "\n",
    "# Loop through all indices\n",
    "for i in range(len(df_dst_port)):\n",
    "    # Create dictionary for each row\n",
    "    flow_row = {\n",
    "        'dst_port': df_dst_port[i],\n",
    "        'dst_asn': df_dst_asn[i],\n",
    "        'quic_ver': quic_ver,\n",
    "        'dur': df_dur[i],\n",
    "        'ratio': df_ratio[i],\n",
    "        'flow_pkt_rate': df_flow_pkt[i],\n",
    "        'flow_byte_rate': df_flow_bytes[i],\n",
    "        'total_pkts': df_tot_pkt[i],\n",
    "        'total_bytes': df_tot_bytes[i],\n",
    "        'max_bytes': df_max_bytes[i],\n",
    "        'min_bytes': df_min_bytes[i],\n",
    "        'ave_bytes': df_ave_bytes[i],\n",
    "        'std_bytes': df_std_bytes[i],\n",
    "        'var_bytes': df_var_bytes[i],\n",
    "        'fwd_pkts': df_fwd_pkt[i],\n",
    "        'fwd_bytes': df_fwd_bytes[i],\n",
    "        'max_fwd_bytes': df_max_fwd_bytes[i],\n",
    "        'min_fwd_bytes': df_min_fwd_bytes[i],\n",
    "        'ave_fwd_bytes': df_ave_fwd_bytes[i],\n",
    "        'std_fwd_bytes': df_std_fwd_bytes[i],\n",
    "        'var_fwd_bytes': df_var_fwd_bytes[i],\n",
    "        'rev_pkts': df_rev_pkt[i],\n",
    "        'rev_bytes': df_rev_bytes[i],\n",
    "        'max_rev_bytes': df_max_rev_bytes[i],\n",
    "        'min_rev_bytes': df_min_rev_bytes[i],\n",
    "        'ave_rev_bytes': df_ave_rev_bytes[i],\n",
    "        'std_rev_bytes': df_std_rev_bytes[i],\n",
    "        'var_rev_bytes': df_var_rev_bytes[i],\n",
    "        'max_iat': df_max_iat[i],\n",
    "        'min_iat': df_min_iat[i],\n",
    "        'ave_iat': df_ave_iat[i],\n",
    "        'std_iat': df_std_iat[i],\n",
    "        'var_iat': df_var_iat[i],\n",
    "        'fwd_dur': df_fwd_dur[i],\n",
    "        'max_fwd_iat': df_max_fwd_iat[i],\n",
    "        'min_fwd_iat': df_min_fwd_iat[i],\n",
    "        'ave_fwd_iat': df_ave_fwd_iat[i],\n",
    "        'std_fwd_iat': df_std_fwd_iat[i],\n",
    "        'var_fwd_iat': df_var_fwd_iat[i],\n",
    "        'rev_dur': df_rev_dur[i],\n",
    "        'max_rev_iat': df_max_rev_iat[i],\n",
    "        'min_rev_iat': df_min_rev_iat[i],\n",
    "        'ave_rev_iat': df_ave_rev_iat[i],\n",
    "        'std_rev_iat': df_std_rev_iat[i],\n",
    "        'var_rev_iat': df_var_rev_iat[i],\n",
    "        'label': df_label[i]\n",
    "    }\n",
    "    flow_rows.append(flow_row)\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "new_flow_df = pd.DataFrame(flow_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./benign_flow/benign.csv\"\n",
    "\n",
    "# Write the updated DataFrame back to the CSV file\n",
    "new_flow_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to inspect DataFrame contents and diagnose issues\n",
    "\n",
    "# Check basic DataFrame info\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "print(\"\\nSample of first 5 rows:\")\n",
    "print(df.head().to_string())\n",
    "\n",
    "# Check for commas in IP fields\n",
    "src_ip_with_commas = df[df['SRC_IP'].str.contains(',', na=False)]\n",
    "dst_ip_with_commas = df[df['DST_IP'].str.contains(',', na=False)]\n",
    "\n",
    "print(f\"\\nRows with commas in SRC_IP: {len(src_ip_with_commas)}\")\n",
    "print(f\"Rows with commas in DST_IP: {len(dst_ip_with_commas)}\")\n",
    "\n",
    "if len(src_ip_with_commas) > 0:\n",
    "    print(\"\\nSample of rows with commas in SRC_IP:\")\n",
    "    print(src_ip_with_commas.head().to_string())\n",
    "    \n",
    "    # Count unique values with commas\n",
    "    unique_problematic_src_ips = src_ip_with_commas['SRC_IP'].unique()\n",
    "    print(f\"\\nUnique problematic SRC_IP values ({len(unique_problematic_src_ips)}):\")\n",
    "    for ip in unique_problematic_src_ips[:10]:  # Show first 10\n",
    "        print(f\"  - {ip}\")\n",
    "    if len(unique_problematic_src_ips) > 10:\n",
    "        print(f\"  ... and {len(unique_problematic_src_ips) - 10} more\")\n",
    "\n",
    "# Check specific comparison that's failing\n",
    "ipsrc = \"10.10.3.10\"\n",
    "problem_rows = df[df['SRC_IP'].str.startswith(ipsrc) & (df['SRC_IP'] != ipsrc)]\n",
    "print(f\"\\nRows where SRC_IP starts with '{ipsrc}' but isn't exactly '{ipsrc}': {len(problem_rows)}\")\n",
    "if len(problem_rows) > 0:\n",
    "    print(\"\\nSample of these problematic rows:\")\n",
    "    print(problem_rows.head().to_string())\n",
    "\n",
    "# Compare string values and lengths\n",
    "if len(problem_rows) > 0:\n",
    "    example = problem_rows['SRC_IP'].iloc[0]\n",
    "    print(f\"\\nDetailed comparison:\")\n",
    "    print(f\"Expected: '{ipsrc}' (length: {len(ipsrc)})\")\n",
    "    print(f\"Actual  : '{example}' (length: {len(example)})\")\n",
    "    print(f\"Equal?  : {ipsrc == example}\")\n",
    "    \n",
    "    # Check character by character\n",
    "    print(\"\\nCharacter by character comparison:\")\n",
    "    for i in range(max(len(ipsrc), len(example))):\n",
    "        if i < len(ipsrc) and i < len(example):\n",
    "            match = ipsrc[i] == example[i]\n",
    "            print(f\"Position {i}: '{ipsrc[i]}' vs '{example[i]}' - Match: {match}\")\n",
    "        elif i < len(ipsrc):\n",
    "            print(f\"Position {i}: '{ipsrc[i]}' vs (no character) - No match\")\n",
    "        else:\n",
    "            print(f\"Position {i}: (no character) vs '{example[i]}' - No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source IP\n",
    "ipsrc = df[\"SRC_IP\"].iloc[0]\n",
    "portsrc = df[\"SRC_PORT\"].iloc[0]\n",
    "cat = \"Streaming\"\n",
    "\n",
    "flow = []\n",
    "\n",
    "df[\"true_dest\"] = df.apply(lambda row: row[\"DST_IP\"] if row[\"SRC_IP\"] == ipsrc else row[\"SRC_IP\"], axis=1)\n",
    "df[\"group\"] = (df[\"true_dest\"] != df[\"true_dest\"].shift()).cumsum()\n",
    "\n",
    "for group, group_df in df.groupby(\"group\"):\n",
    "    num_subgroups = (len(group_df) + 29) // 30\n",
    "    subgroups = [group_df.iloc[i * 30:(i+1) * 30] for i in range(num_subgroups)]\n",
    "    \n",
    "    # print(f\"Group {group}:\")\n",
    "    # print(group_df)\n",
    "\n",
    "    for  subgroup in subgroups:\n",
    "        ppi_dir = []\n",
    "        ipdst = subgroup[\"true_dest\"].iloc[0]  # The unique normalized destination for this subgroup\n",
    "        portdst = subgroup[\"DST_PORT\"].iloc[0] \n",
    "\n",
    "        ppi_time = [0]\n",
    "        ppi_size = [int(subgroup[\"BYTES\"].iloc[0])]\n",
    "        for i in range(1, len(subgroup)):\n",
    "            # Calculate the time difference between consecutive packets\n",
    "            duration = int((subgroup[\"DURATION\"].iloc[i] - subgroup[\"DURATION\"].iloc[i - 1]) * 1000)\n",
    "            ppi_time.append(duration)\n",
    "            ppi_size.append(int(subgroup[\"BYTES\"].iloc[i]))\n",
    "\n",
    "\n",
    "\n",
    "        for _, row in subgroup.iterrows():\n",
    "            if row[\"SRC_IP\"] == ipsrc and row[\"DST_IP\"] == ipdst:\n",
    "                ppi_dir.append(1)\n",
    "            elif row[\"SRC_IP\"] == ipdst  and row[\"DST_IP\"] == ipsrc:\n",
    "                ppi_dir.append(-1)\n",
    "            else:\n",
    "                ppi_dir.append(0)\n",
    "\n",
    "            bytes_fromsrc = int(subgroup.loc[(subgroup[\"SRC_IP\"] == ipsrc) & (subgroup[\"DST_IP\"] == ipdst),\"BYTES\"].sum())\n",
    "            bytes_rev = int(subgroup.loc[(subgroup[\"SRC_IP\"] == ipdst) & (subgroup[\"DST_IP\"] == ipsrc),\"BYTES\"].sum())\n",
    "            packets = int(subgroup.loc[(subgroup[\"SRC_IP\"] == ipsrc) & (subgroup[\"DST_IP\"] == ipdst),\"BYTES\"].count())\n",
    "            packets_rev = int(subgroup.loc[(subgroup[\"SRC_IP\"] == ipdst) & (subgroup[\"DST_IP\"] == ipsrc),\"BYTES\"].count())\n",
    "            ppi_len = len(ppi_dir)\n",
    "\n",
    "            ppi_rtt = 0\n",
    "            in_group = False\n",
    "\n",
    "            # Iterate through the list\n",
    "            for value in ppi_dir:\n",
    "                if value == -1:  # Start or continue a group of -1's\n",
    "                    if not in_group:\n",
    "                        in_group = True  # Beginning of a group\n",
    "                elif value == 1:  # Start or continue a group of 1's\n",
    "                    if not in_group:\n",
    "                        in_group = True  # Beginning of a group\n",
    "                    elif in_group:\n",
    "                        ppi_rtt += 1\n",
    "                        in_group = False  # Reset for the next group\n",
    "\n",
    "            # Handle the case where the list ends with a valid pair\n",
    "            if in_group:\n",
    "                ppi_rtt += 1\n",
    "\n",
    "        # print(\"Subgroup:\")\n",
    "        # print(subgroup)\n",
    "\n",
    "        dur = round(subgroup[\"DURATION\"].max() - subgroup[\"DURATION\"].min(), ndigits=6)\n",
    "\n",
    "        #flow.append([ipsrc, ipdst, asndb.lookup(ipdst)[0], portsrc, portdst, 1, sni, dur, bytes_fromsrc, bytes_rev, packets, packets_rev, ppi_len, ppi_rtt, cat, [ppi_time, ppi_dir, ppi_size]])\n",
    "        flow.append([portdst, asndb.lookup(ipdst)[0], portsrc, portdst, 1, sni, dur, bytes_fromsrc, bytes_rev, packets, packets_rev, ppi_len, ppi_rtt, cat, [ppi_time, ppi_dir, ppi_size]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 270 CSV files in 9 folders.\n",
      "Merging files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 56/270 [00:02<00:09, 21.68it/s] C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 23%|██▎       | 61/270 [00:03<00:15, 13.77it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 24%|██▍       | 65/270 [00:04<00:22,  8.97it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 26%|██▌       | 70/270 [00:05<00:26,  7.48it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 27%|██▋       | 72/270 [00:06<00:32,  6.02it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 27%|██▋       | 73/270 [00:06<00:32,  6.04it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 27%|██▋       | 74/270 [00:06<00:32,  6.08it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 28%|██▊       | 75/270 [00:06<00:39,  4.99it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 28%|██▊       | 76/270 [00:07<00:45,  4.24it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 29%|██▊       | 77/270 [00:07<00:54,  3.56it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 29%|██▉       | 78/270 [00:08<01:03,  3.03it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 29%|██▉       | 79/270 [00:08<01:08,  2.78it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 30%|██▉       | 80/270 [00:08<01:04,  2.96it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 30%|███       | 81/270 [00:09<01:02,  3.05it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 31%|███       | 83/270 [00:09<01:02,  2.98it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 31%|███       | 84/270 [00:10<01:05,  2.84it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 31%|███▏      | 85/270 [00:10<01:03,  2.91it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 32%|███▏      | 86/270 [00:10<00:54,  3.40it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 32%|███▏      | 87/270 [00:11<00:51,  3.56it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 33%|███▎      | 88/270 [00:11<00:54,  3.33it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 33%|███▎      | 89/270 [00:11<00:59,  3.05it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 56%|█████▌    | 150/270 [00:14<00:04, 25.65it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 59%|█████▊    | 158/270 [00:15<00:08, 12.71it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 59%|█████▉    | 160/270 [00:15<00:10, 10.88it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 62%|██████▏   | 168/270 [00:16<00:11,  8.87it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 63%|██████▎   | 169/270 [00:16<00:12,  7.94it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 63%|██████▎   | 170/270 [00:16<00:13,  7.23it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 63%|██████▎   | 171/270 [00:17<00:14,  6.64it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 64%|██████▎   | 172/270 [00:17<00:15,  6.49it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 64%|██████▍   | 173/270 [00:17<00:19,  4.93it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 64%|██████▍   | 174/270 [00:17<00:22,  4.26it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 65%|██████▌   | 176/270 [00:18<00:18,  5.14it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 66%|██████▌   | 177/270 [00:18<00:18,  5.07it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      " 66%|██████▌   | 178/270 [00:18<00:17,  5.38it/s]C:\\Users\\John Raphael Mundo\\AppData\\Local\\Temp\\ipykernel_10292\\3361370010.py:39: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, chunksize=100000):\n",
      "100%|██████████| 270/270 [00:20<00:00, 13.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge completed in 20.41 seconds.\n",
      "Master file saved as: c:\\Users\\John Raphael Mundo\\Desktop\\Capstone Repo\\QUIC\\Data_Collection\\master_file.csv\n",
      "Master file size: 506.27 MB\n",
      "Total rows: 8,671,986\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def merge_csv_files(parent_folder, output_file='master_file.csv'):\n",
    "    \"\"\"\n",
    "    Merge all CSV files from multiple folders into one master file.\n",
    "    \n",
    "    Args:\n",
    "        parent_folder (str): Path to the parent folder containing subfolders with CSV files\n",
    "        output_file (str): Path where the merged CSV file will be saved\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get all CSV files from all subfolders\n",
    "    all_csv_files = glob.glob(os.path.join(parent_folder, '**', '*.csv'), recursive=True)\n",
    "    \n",
    "    if not all_csv_files:\n",
    "        print(f\"No CSV files found in {parent_folder} or its subfolders.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(all_csv_files)} CSV files in {len(set(os.path.dirname(f) for f in all_csv_files))} folders.\")\n",
    "    \n",
    "    # Read the first file to get the header\n",
    "    first_df = pd.read_csv(all_csv_files[0])\n",
    "    header = first_df.columns.tolist()\n",
    "    \n",
    "    # Write the header to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_output:\n",
    "        pd.DataFrame(columns=header).to_csv(f_output, index=False)\n",
    "    \n",
    "    # Process each file and append to master file\n",
    "    print(\"Merging files...\")\n",
    "    for file_path in tqdm(all_csv_files):\n",
    "        try:\n",
    "            # Read each CSV file in chunks to handle large files efficiently\n",
    "            for chunk in pd.read_csv(file_path, chunksize=100000):\n",
    "                # Append to the master file without writing the header again\n",
    "                chunk.to_csv(output_file, mode='a', header=False, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Final processing time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Merge completed in {elapsed_time:.2f} seconds.\")\n",
    "    print(f\"Master file saved as: {os.path.abspath(output_file)}\")\n",
    "    \n",
    "    # Get some basic stats about the merged file\n",
    "    try:\n",
    "        file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "        row_count = sum(1 for _ in open(output_file, 'r')) - 1  # Subtract 1 for header\n",
    "        print(f\"Master file size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"Total rows: {row_count:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file stats: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the parent folder containing subfolders with CSV files\n",
    "    parent_folder = input(\"Enter the path to the parent folder: \").strip()\n",
    "    \n",
    "    # Specify output file path\n",
    "    output_file = input(\"Enter the path for the master file (or press Enter for 'master_file.csv'): \").strip()\n",
    "    if not output_file:\n",
    "        output_file = \"master_file.csv\"\n",
    "    \n",
    "    # Run the merge\n",
    "    merge_csv_files(parent_folder, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set source IP, web service category, and label\n",
    "ipsrc = \"10.10.3.10\"\n",
    "lst_asn = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    value = row['SRC_IP']\n",
    "    if value.startswith(ipsrc):\n",
    "        try:\n",
    "            lst_asn.append(asndb.lookup(row['DST_IP'])[0])\n",
    "        except:\n",
    "            false_ip = row['DST_IP'].split(\",\")[0]\n",
    "            lst_asn.append(asndb.lookup(false_ip)[0])\n",
    "            print(f\"ip: {false_ip} -> asn: {asndb.lookup(false_ip)[0]}\")\n",
    "    elif value.startswith(ipsrc) == False:\n",
    "        try:\n",
    "            lst_asn.append(asndb.lookup(row['SRC_IP'])[0])\n",
    "        except:\n",
    "            false_ip = row['SRC_IP'].split(\",\")[0]\n",
    "            lst_asn.append(asndb.lookup(false_ip)[0])\n",
    "            print(f\"ip: {false_ip} -> asn: {asndb.lookup(false_ip)[0]}\")\n",
    "        \n",
    "    if index % 1000 == 0:\n",
    "        print(f\"{index / len(df) * 100:.2f}%\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
